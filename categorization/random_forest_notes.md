# Random forest notes

## Creating a binary decision tree

Note that binary decision trees are usually preferable to a decision tree created by multiway splitting, as it is less
likely to result in **overfitting**. Overfitting is "the production of an analysis that corresponds too closely or exactly to
a particular set of data, and may therefore fail to fit additional data or predict future observations reliably"
([Wikipedia][1]). This is because it breaks the dataset into too many subsets too quickly.  

### 1. Splitting the dataset
*Note that for k classes, there are 2^(k-1) - 1 possible splits ([Hoare][3]), which is terrible. My k value 
is however many categories I am trying to sort the subreddits into.* <br/>
<br/>
Each split of the dataset is made with respect to a single attribute (i.e. whether or not a word is present in the 
subreddit name). Then, how 'useful' each split is is determined using the Gini index (see below). The 'best'
split is the one used in the tree ([Brownlee][2]). <br/>
<br/>
All of my predictor variables are categorical, and have only two possible states (true/false), which is nice because 
that's conducive to creating a BINARY decision tree (as opposed to multiway). 

### 2. Calculate the Gini index
The Gini index is used to evaluate how effective each split of the dataset was at categorizing the data. It is essentially
the probability of an item in a category being chosen multiplied by the probability of the item being incorrectly
categorized (the probability of not choosing an item from that category); these values are summed for each category in a
given node) ([Wikipedia][4]). This simplifies to the formula described in the code below ([Brownlee][2]). <br/>

```
proportion1 = count(category_1)/count(items_in_split)
proportion2 = count(category_2)/count(items_in_split)
.
.
.
proportionn = count(category_n)/count(items_in_split)                    
gini_impurity_node1 = 1 - (sum(proportion1**2, proportion2**2, ..., proportionn**2))
```
The Gini index must then be weighted according to the size of each group for which it was calculated. <br/>

```
gini_impurity_node1_weighted = gini_impurity_node1*(node_size/total_samples)
gini_impurity_node2_weighted = gini_impurity_node2*(node_size/total_samples)
```
For a binary decision tree, there would only be two nodes to consider. <br/>
<br/>
To calculate the Gini impurity for an entire potential split point, sum the weighted Gini impurities of each individual node
generated by the split point. These values can then be compared between possible split points; the lower the Gini score,
the better the split point (a Gini score of zero indicates that everything is correctly sorted, and a score of 1 indicates
that your split is really fucking useless).  

### 3. Growing the tree

Basically, apply the aforementioned processes to each new node of the tree, choosing the best attribute on which to split
the branch with each successive application (use **recursion** to continually split the dataset). This is pretty 
straightforward, and I should be able to figure it out by myself. Maybe write this part in C++ (for speed) and use it
as practice for importing C++ modules. <br/>
<br/>
A parent node can give rise to one of three cases: no children (terminal node), one child (all elements in the group match
with respect to a certain attribute), or two children (regular split). 

### 4. Terminating the tree

There are three ways to specify the termination of a tree: <br/>
1. Set a maximum tree depth (only let each branch divide a certain number of times; too high of a tree depth
   can lead to overfitting).
2. Set a minimum node size (too small of a minimum node size can lead to subsets which are too specific, also causing
   overfitting).
3. Grow the tree until each node only contains items belonging to a single category.

*What are the downsides of growing the tree until each node contains only elements of a single category? Is it
just that the subsets will get to specific? (overfitting, again)*


## Creating a random forest

### 1. Bootstrap aggregating

'Bootstrapping' "is any test or metric that relies on random sampling with replacement"; this statistical method is called 'bagging' for short ([Wikipedia][5]) (alternatively, there is sub-bagging, which involves choosing sub-groups for testing without replacement). When creating a random forest, each decision tree is trained on a bag of data from the available training data. The typical size of a bag *M* in the with-replacement case is *N*, where *N* is the size of the entire training dataset. Because bag selection is done with replacement, each bag is expected to have around 63% of the training dataset (as some of the data are duplicates). However, tests have been done which show that a bag size of *M* = 0.8*N* is usually better than (and never worse than) *M* = *N* ([Munoz and Suarez][6]). <br/>
<br/>
*Note that an 'ensemble' is the name of a group of bags of data.* 

### 2. Growing the forest

As far as I can tell, you basically just create a decision tree trained on each bag

### 3. Obtaining results


### 3. Assessing prediction accuracy


[1]: https://en.wikipedia.org/wiki/Overfitting
[2]: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/
[3]: https://www.displayr.com/how-is-splitting-decided-for-decision-trees/
[4]: https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
[5]: https://en.wikipedia.org/wiki/Bootstrapping_(statistics)
[6]: https://www.sciencedirect.com/science/article/abs/pii/S003132030900212X?via%3Dihub
